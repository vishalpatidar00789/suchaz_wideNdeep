{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of wideNdeep.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[{"file_id":"1CixPJT7hI0Cqzy9ow7EHo_6RemzNrt3J","timestamp":1523991868063}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"5gBlbfV1w1MT","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import argparse\n","import os\n","import shutil\n","import sys\n","\n","import tensorflow as tf  # pylint: disable=g-bad-import-order\n","\n","from official.utils.arg_parsers import parsers\n","from official.utils.logs import hooks_helper\n","from official.utils.misc import model_helpers\n","\n","# define columns of your input dataset of csv\n","_CSV_COLUMNS = [\n","    'gender', 'age', 'lct', 'hct', 'root',\n","    'category'\n","]\n","\n","# init column default values\n","_CSV_COLUMN_DEFAULTS = [[''], [''], [''], [''],[''], ['']]\n","\n","# define an object that gives information of training and validation records\n","_NUM_EXAMPLES = {\n","    'train': 70000,\n","    'validation': 30000,\n","}\n","\n","LOSS_PREFIX = {'wide': 'linear/', 'deep': 'dnn/'}\n","\n","\n","# define and build wide and deep columns\n","def build_model_columns():\n","  # define continuous columns\n","  age = tf.feature_column.numeric_column('age')\n","  # define categorical columns\n","  gender = tf.feature_column.categorical_column_with_vocabulary_list(\n","    'gender', ['M', 'F']\n","  )\n","  root = tf.feature_column.categorical_column_with_vocabulary_list(\n","    'root', ['T1', 'T2', 'T3','T4','T5','T6','T7','T8','T9']\n","  )\n","  # define categorical columns with fix hash bucket\n","  lct = tf.feature_column.categorical_column_with_hash_bucket(\n","    'lct', hash_bucket_size=1500\n","  )\n","  hct  = tf.feature_column.categorical_column_with_hash_bucket(\n","    'hct', hash_bucket_size=100\n","  )\n","  category = tf.feature_column.categorical_column_with_hash_bucket(\n","    'category', hash_bucket_size=100000\n","  )\n","  \n","  # trasformation of age feature\n","  age_buckets = tf.feature_column.bucketized_column(\n","    'age', [10, 18, 35, 100]\n","  )\n","  \n","  # define base columns\n","  base_columns = [\n","      gender, age, lct, hct, root\n","  ]\n","  \n","  # define cross columns\n","  crossed_columns = [\n","      tf.feature_column.crossed_column(\n","        ['gender', 'lct'], hash_bucket_size = 1500\n","      ),\n","      tf.feature_column.crossed_column(\n","        ['age_buckets','lct'], hash_bucket_size = 1500\n","      ),\n","      tf.feature_column.crossed_column(\n","        ['gender','age_buckets','lct'], hash_bucket_size = 1500\n","      ),\n","      tf.feature_column.crossed_column(\n","        ['gender','hct'], hash_bucket_size = 100\n","      ),\n","      tf.feature_column.crossed_column(\n","        ['age_buckets','hct'], hash_bucket_size = 100\n","      ),\n","      tf.feature_column.crossed_column(\n","        ['gender','age_buckets','hct'], hash_bucket_size = 100\n","      ),\n","      tf.feature_column.crossed_column(\n","        ['gender','root'], hash_bucket_size = 100\n","      ),\n","      tf.feature_column.crossed_column(\n","        ['age_buckets','root'], hash_bucket_size = 100\n","      ),\n","      tf.feature_column.crossed_column(\n","        ['gender','age_buckets','root'], hash_bucket_size = 100\n","      )\n","    \n","  ]    \n","  \n","  # wide columns for wide network\n","  wide_columns = base_columns + crossed_columns\n","  \n","  # define deep columns for deep network\n","  deep_columns = [\n","      tf.feature_column.indicator_column(gender), \n","      age_buckets, \n","      lct, hct, root\n","  ]\n","  \n","  return wide_columns, deep_columns\n","\n","def build_estimator(model_dir, model_type)\n","  wide_columns, deep_columns = build_model_columns()\n","  hidden_units = [100, 75, 50, 25]\n","\n","  # Create a tf.estimator.RunConfig to ensure the model is run on CPU, which\n","  # trains faster than GPU for this model.\n","  run_config = tf.estimator.RunConfig().replace(\n","      session_config=tf.ConfigProto(device_count={'GPU': 0}))\n","\n","  if model_type == 'wide':\n","    return tf.estimator.LinearClassifier(\n","        model_dir=model_dir,\n","        feature_columns=wide_columns,\n","        config=run_config)\n","  elif model_type == 'deep':\n","    return tf.estimator.DNNClassifier(\n","        model_dir=model_dir,\n","        feature_columns=deep_columns,\n","        hidden_units=hidden_units,\n","        config=run_config)\n","  else:\n","    return tf.estimator.DNNLinearCombinedClassifier(\n","        model_dir=model_dir,\n","        linear_feature_columns=wide_columns,\n","        dnn_feature_columns=deep_columns,\n","        dnn_hidden_units=hidden_units,\n","        config=run_config)\n","  \n","def input_fn(data_file, num_epochs, shuffle, batch_size):\n","  # Generate an input function for the Estimator.\n","  assert tf.gfile.Exists(data_file), (\n","      '%s not found. Please make sure you have run data_download.py and '\n","      'set the --data_dir argument to the correct path.' % data_file)\n","\n","  def parse_csv(value):\n","    print('Parsing', data_file)\n","    columns = tf.decode_csv(value, record_defaults=_CSV_COLUMN_DEFAULTS)\n","    features = dict(zip(_CSV_COLUMNS, columns))\n","    labels = features.pop('category')\n","    return features, labels\n","\n","  # Extract lines from input files using the Dataset API.\n","  dataset = tf.data.TextLineDataset(data_file)\n","\n","  if shuffle:\n","    dataset = dataset.shuffle(buffer_size=_NUM_EXAMPLES['train'])\n","\n","  dataset = dataset.map(parse_csv, num_parallel_calls=5)\n","\n","  # We call repeat after shuffling, rather than before, to prevent separate\n","  # epochs from blending together.\n","  dataset = dataset.repeat(num_epochs)\n","  dataset = dataset.batch(batch_size)\n","  \n","  return dataset\n","\n","def main(argv):\n","  parser = WideDeepArgParser()\n","  flags = parser.parse_args(args=argv[1:])\n","\n","  # Clean up the model directory if present\n","  shutil.rmtree(flags.model_dir, ignore_errors=True)\n","  model = build_estimator(flags.model_dir, flags.model_type)\n","\n","  train_file = os.path.join(flags.data_dir, 'suchaz_train.data')\n","  test_file = os.path.join(flags.data_dir, 'suchaz.test')\n","\n","  # Train and evaluate the model every `flags.epochs_between_evals` epochs.\n","  def train_input_fn():\n","    return input_fn(\n","        train_file, flags.epochs_between_evals, True, flags.batch_size)\n","\n","  def eval_input_fn():\n","    return input_fn(test_file, 1, False, flags.batch_size)\n","\n","  loss_prefix = LOSS_PREFIX.get(flags.model_type, '')\n","  train_hooks = hooks_helper.get_train_hooks(\n","      flags.hooks, batch_size=flags.batch_size,\n","      tensors_to_log={'average_loss': loss_prefix + 'head/truediv',\n","                      'loss': loss_prefix + 'head/weighted_loss/Sum'})\n","\n","  # Train and evaluate the model every `flags.epochs_between_evals` epochs.\n","  for n in range(flags.train_epochs // flags.epochs_between_evals):\n","    model.train(input_fn=train_input_fn, hooks=train_hooks)\n","    results = model.evaluate(input_fn=eval_input_fn)\n","\n","    # Display evaluation metrics\n","    print('Results at epoch', (n + 1) * flags.epochs_between_evals)\n","    print('-' * 60)\n","\n","    for key in sorted(results):\n","      print('%s: %s' % (key, results[key]))\n","\n","    if model_helpers.past_stop_threshold(\n","        flags.stop_threshold, results['accuracy']):\n","      break\n","\n","class WideDeepArgParser(argparse.ArgumentParser):\n","  # Argument parser for running the wide deep model.\n","\n","  def __init__(self):\n","    super(WideDeepArgParser, self).__init__(parents=[\n","        parsers.BaseParser(multi_gpu=False, num_gpu=False)])\n","    self.add_argument(\n","        '--model_type', '-mt', type=str, default='wide_deep',\n","        choices=['wide', 'deep', 'wide_deep'],\n","        help='[default %(default)s] Valid model types: wide, deep, wide_deep.',\n","        metavar='<MT>')\n","    self.set_defaults(\n","        data_dir='/tmp/suchaz_data',\n","        model_dir='/tmp/suchaz_model',\n","        train_epochs=40,\n","        epochs_between_evals=2,\n","        batch_size=40)\n","\n","if __name__ == '__main__':\n","  tf.logging.set_verbosity(tf.logging.INFO)\n","  main(argv=sys.argv)"],"execution_count":0,"outputs":[]}]}